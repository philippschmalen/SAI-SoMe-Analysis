{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from os.path import basename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from webbot import Browser \n",
    "\n",
    "url = 'https://anchor.fm/dashboard'\n",
    "username = 'cherylschwahn@icloud.com'\n",
    "password = 'Sean1998!'\n",
    "\n",
    "web = Browser()\n",
    "web.go_to(url) \n",
    "\n",
    "web.type(username , into='Email')\n",
    "web.type(password , into='Password')\n",
    "\n",
    "web.click('Log In')\n",
    "\n",
    "web.click('No thanks')\n",
    "\n",
    "# web.click('Weekly')\n",
    "\n",
    "# web.click('Daily')\n",
    "\n",
    "# web.click('Export current view as CSV')\n",
    "\n",
    "# TODO: demographics - m/f/d share and age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get html\n",
    "requests_get = requests.get(url).text\n",
    "soup = BeautifulSoup(requests_get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-08 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# unix timestamp\n",
    "date_start = 1602115200 # 2020-10-08\n",
    "interval_weekly = 604800\n",
    "interval_daily = \n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# if you encounter a \"year is out of range\" error the timestamp\n",
    "# may be in milliseconds, try `ts /= 1000` in that case\n",
    "print(datetime.utcfromtimestamp(date_start).strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url does not seem to change for different logins\n",
    "weekly = \"https://anchor.fm/api/proxy/v3/analytics/station/webStationId:36f5b4ec/plays?timeInterval=604800&timeRangeStart=1602115200&timeRangeEnd=1610582400&csvFilename=RWYToSustainability_TotalPlays_2020-10-08_to_2021-01-14.csv\"\n",
    "daily = \"https://anchor.fm/api/proxy/v3/analytics/station/webStationId:36f5b4ec/plays?timeInterval=86400&timeRangeStart=1602115200&timeRangeEnd=1610582400&csvFilename=b-10-08_to_2021-01-14.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "req = requests.get(weekly)\n",
    "url_content = req.content\n",
    "csv_file = open('downloaded.csv', 'wb')\n",
    "\n",
    "csv_file.write(url_content)\n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in soup.find_all(text='Download CSV'): \n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def webbot_init(url):\n",
    "    \"\"\"Initialize webbot, return browser object\"\"\"\n",
    "    # init webbot browser\n",
    "    web = Browser()\n",
    "    web.go_to(url)\n",
    "    \n",
    "    return web\n",
    "\n",
    "def wait_until(somepredicate, timeout, period=0.25, *args, **kwargs):\n",
    "    \"\"\"Wait until condition is satisfied \"\"\"\n",
    "    mustend = time() + timeout\n",
    "    while time() < mustend:\n",
    "        if somepredicate: return True\n",
    "        sleep(period)\n",
    "    return False\n",
    "\n",
    "def login(webbrowser, url, pw, email):\n",
    "    \"\"\"Login with webbot in URL\"\"\"   \n",
    "    # wait until login fiels exist\n",
    "    login_exists = webbrowser.exists(text='Email or Username', id='Input_UserName', loose_match=False)\n",
    "    wait_until(login_exists, timeout=5)\n",
    "    \n",
    "    # login with username and password\n",
    "    webbrowser.type(email, into='Email or Username')#, id=\"Input_UserName\")\n",
    "    webbrowser.type(pw, into='Password')#, id=\"Input_Password\") \n",
    "    \n",
    "    # click sign in button\n",
    "    webbrowser.click('SIGN IN')\n",
    "    \n",
    "def get_html(webbrowser):\n",
    "    \"\"\"Obtain HTML source code of current page\"\"\"\n",
    "    # get page content\n",
    "    html = webbrowser.get_page_source() # get source code\n",
    "    soup = BeautifulSoup(html) # parse HTML with BS\n",
    "    \n",
    "    return soup\n",
    "\n",
    "\n",
    "\n",
    "# credentials\n",
    "login_url = \"https://app.wodify.com/SignIn/Login?OriginalURL=&RequiresConfirm=false\"\n",
    "pw = 'Papasierra1'\n",
    "email = 'philippschmalen@gmail.com'\n",
    "# main wod page\n",
    "wod_url = 'https://app.wodify.com/WOD/WODEntry.aspx'\n",
    "\n",
    "# init webbrowser\n",
    "webbrowser = webbot_init(login_url)\n",
    "# login to page\n",
    "login(webbrowser, login_url, pw, email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape WODs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sepcify time window\n",
    "years = 3\n",
    "days_backwards = years*365\n",
    "days_backwards_continued = (years+2)*365\n",
    "\n",
    "for t in range(days_backwards,days_backwards+days_backwards_continued): #changed range to \n",
    "    day = date.today()-timedelta(days=t)\n",
    "    print(day)\n",
    "    webbrowser.type(day.strftime(\"%d-%m-%Y\"), id='AthleteTheme_wtLayoutNormal_block_wtSubNavigation_wttxtDate') #into=date.today().strftime(\"%Y-%m-%d\"))\n",
    "    \n",
    "    # press enter\n",
    "    webbrowser.press(webbrowser.Key.ENTER)\n",
    "    \n",
    "    # wait a bit\n",
    "    sleep(1.6)\n",
    "    \n",
    "    # get html source code\n",
    "    html = webbrowser.get_page_source() # get source code\n",
    "    soup = BeautifulSoup(html) # parse HTML with BS\n",
    "    # get text\n",
    "    text_all = soup.get_text(separator=\"\\n\")\n",
    "\n",
    "    ## isolate text body\n",
    "    # body is enclosed by '\\xa0\\n' and '\\n\\xa0'\n",
    "    # replace those parts with # to split the string\n",
    "    text_all = text_all.replace(\"\\xa0\\n\", '#').replace(\"\\n\\xa0\", '#')\n",
    "    text_body1 = text_all.split(\"#\")[2]\n",
    "    text_body2 = text_all.split(\"#\")[3]\n",
    "\n",
    "    # select section titles with div class=''\n",
    "    section_titles = [section_title.text for section_title in soup.find_all(\"div\", class_=\"section_title\")]\n",
    "    # get workout text\n",
    "    try:\n",
    "        workout1 = re.split(\"(\\{})\".format(section_titles[0]), text_body1)\n",
    "        workout2 = re.split(\"(\\{})\".format(section_titles[0]), text_body2)\n",
    "    except:\n",
    "        workout1 = 'failed'\n",
    "        workout2 = 'failed'\n",
    "    \n",
    "    # store page content in DataFrame as row \n",
    "    row = pd.DataFrame([day, section_titles, workout1, workout2]).T\n",
    "    \n",
    "    # export to CSV\n",
    "    row.to_csv('../data/raw/wod_content_years_3_to_5.csv', \n",
    "              header=False, \n",
    "              index=False,\n",
    "               mode='a')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
